{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "the transformer model was made with the help of generative ai"
      ],
      "metadata": {
        "id": "EnjqZNbZJmOv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ieq0McDzRn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15a8c971-c232-4620-d799-d5803b178835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (2.0.2)\n",
            "Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting focal_loss_torch\n",
            "  Downloading focal_loss_torch-0.1.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from focal_loss_torch) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from focal_loss_torch) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->focal_loss_torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->focal_loss_torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->focal_loss_torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->focal_loss_torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->focal_loss_torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->focal_loss_torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->focal_loss_torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->focal_loss_torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->focal_loss_torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->focal_loss_torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->focal_loss_torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->focal_loss_torch) (3.0.2)\n",
            "Downloading focal_loss_torch-0.1.2-py3-none-any.whl (4.5 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, focal_loss_torch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed focal_loss_torch-0.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "# !pip install d2l==1.0.3\n",
        "# !pip install torch==2.5.1\n",
        "! pip install biopython\n",
        "!pip install wandb -qU\n",
        "!pip install focal_loss_torch\n",
        "# !pip install torch==2.5.1  # Install PyTorch first\n",
        "# !pip install numpy==1.24.3  # Install NumPy with a compatible version\n",
        "!pip install transformers\n",
        "# !pip install numpy --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "! git clone https://git.wur.nl/bioinformatics/grs34806-deep-learning-project-data.git\n",
        "os.chdir(\"grs34806-deep-learning-project-data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-Hh8uS163kz",
        "outputId": "6abb86b2-5059-433b-aea9-25b91fbfef79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'grs34806-deep-learning-project-data'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Total 21 (delta 0), reused 0 (delta 0), pack-reused 21 (from 1)\u001b[K\n",
            "Receiving objects: 100% (21/21), 8.74 MiB | 5.61 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
        "from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel\n",
        "import regex as re\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def read(seqfile: str, posfile: str):\n",
        "    \"\"\"\n",
        "    Read sequences and positive labels from files.\n",
        "    seqfile: whitespace-separated lines of <id> <sequence>\n",
        "    posfile: one ID per line for positive examples\n",
        "    Returns: (list of sequences, list of integer labels)\n",
        "    \"\"\"\n",
        "    datalist, annot_ids = [], []\n",
        "    with open(seqfile) as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 2:\n",
        "                continue\n",
        "            pid, seq = parts\n",
        "            annot_ids.append(pid)\n",
        "            datalist.append(seq)\n",
        "    pos_ids = set(line.strip() for line in open(posfile))\n",
        "    labels = [1 if pid in pos_ids else 0 for pid in annot_ids]\n",
        "    return datalist, labels\n",
        "\n",
        "\n",
        "def generate_train_test(seqs, labels, test_prop=0.2, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    n = len(seqs)\n",
        "    idx = np.random.permutation(n)\n",
        "    split = int(n * (1 - test_prop))\n",
        "    return (\n",
        "        [seqs[i] for i in idx[:split]], [labels[i] for i in idx[:split]]\n",
        "    ), (\n",
        "        [seqs[i] for i in idx[split:]], [labels[i] for i in idx[split:]]\n",
        "    )\n",
        "\n",
        "mapaa2num = {aa: i for i, aa in enumerate(list(\"ACDEFGHIKLMNPQRSTVWY\"))}\n",
        "\n",
        "def pad_or_trim(seq: str, size: int, pad_char: str = '_') -> str:\n",
        "    if len(seq) > size:\n",
        "        return seq[:size]\n",
        "    return seq + pad_char * (size - len(seq))\n",
        "\n",
        "def add_spaces(seq: str) -> str:\n",
        "    return ' '.join(list(seq))\n",
        "\n",
        "def tokenize_map(seqs, mapping, non_aa=20):\n",
        "    return [[mapping.get(aa, non_aa) for aa in seq] for seq in seqs]\n",
        "\n",
        "def truncate_pad(ids, max_len, pad_id=20):\n",
        "    if len(ids) >= max_len:\n",
        "        return ids[:max_len]\n",
        "    return ids + [pad_id] * (max_len - len(ids))\n",
        "\n",
        "def build_seq_array(id_lists, max_len):\n",
        "    return torch.tensor([truncate_pad(ids, max_len) for ids in id_lists], dtype=torch.long)\n",
        "\n",
        "\n",
        "def load_data(batch_size: int,\n",
        "              seqs: list,\n",
        "              labels: list,\n",
        "              max_len: int,\n",
        "              tokenizer=None):\n",
        "    if tokenizer:\n",
        "        in_ids, in_mask, in_labels = [], [], []\n",
        "        for seq, lbl in zip(seqs, labels):\n",
        "            s = pad_or_trim(seq, max_len)\n",
        "            s = re.sub(r\"[UZOB]\", \"X\", s)\n",
        "            enc = tokenizer(add_spaces(s), return_tensors='pt',\n",
        "                            padding='max_length', truncation=True,\n",
        "                            max_length=max_len)\n",
        "            in_ids.append(enc['input_ids'])\n",
        "            in_mask.append(enc['attention_mask'])\n",
        "            in_labels.append(lbl)\n",
        "        X = torch.cat(in_ids, dim=0)\n",
        "        M = torch.cat(in_mask, dim=0)\n",
        "        Y = torch.tensor(in_labels, dtype=torch.float32).unsqueeze(1)\n",
        "        ds = TensorDataset(X, M, Y)\n",
        "        return DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "    else:\n",
        "        ids = tokenize_map(seqs, mapaa2num)\n",
        "        X = build_seq_array(ids, max_len)\n",
        "        Y = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)\n",
        "        cnt = torch.tensor([labels.count(0), labels.count(1)], dtype=torch.float)\n",
        "        w = 1.0 / cnt\n",
        "        sample_w = torch.tensor([w[int(l)] for l in labels])\n",
        "        sampler = WeightedRandomSampler(sample_w, len(sample_w), True)\n",
        "        ds = TensorDataset(X, Y)\n",
        "        return DataLoader(ds, batch_size=batch_size, sampler=sampler)\n",
        "\n",
        "class ProteinTransformerClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_name: str = \"Rostlab/prot_bert\",\n",
        "                 unfreeze_layers: int = 2,\n",
        "                 hidden_dim: int = 64,\n",
        "                 dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        # load any HF model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
        "        self.base = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        # freeze all then unfreeze top layers - #not sure what this does\n",
        "        for p in self.base.parameters(): p.requires_grad = False\n",
        "        if hasattr(self.base, 'encoder'):\n",
        "            layers = self.base.encoder.layer[-unfreeze_layers:]\n",
        "        else:\n",
        "            layers = list(self.base.children())[-unfreeze_layers:]\n",
        "        for layer in layers:\n",
        "            for p in layer.parameters(): p.requires_grad = True\n",
        "\n",
        "        feat = self.base.config.hidden_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(feat, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        out = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls = out.last_hidden_state[:, 0, :]\n",
        "        return self.classifier(self.dropout(cls))\n",
        "\n",
        "# Training loop with metrics & early stopping\n",
        "def train_model(model, train_loader, val_loader, device,\n",
        "                lr=1e-4, weight_decay=1e-4, epochs=20,\n",
        "                clip=1.0, patience=5):\n",
        "    model.to(device)\n",
        "    optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, 'min', patience=2)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    best = float('inf'); wait = 0\n",
        "    for e in range(epochs):\n",
        "        # train\n",
        "        model.train(); tloss=[]; tpr, ttr=[],[]\n",
        "        for batch in train_loader:\n",
        "            if len(batch)==3:\n",
        "                xb, mask, yb = [b.to(device) for b in batch]\n",
        "                logits = model(xb, attention_mask=mask)\n",
        "            else:\n",
        "                xb, yb = [b.to(device) for b in batch]\n",
        "                logits = model(xb)\n",
        "            optim.zero_grad()\n",
        "            loss=loss_fn(logits, yb)\n",
        "            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), clip); optim.step()\n",
        "            tloss.append(loss.item())\n",
        "            preds=(torch.sigmoid(logits)>0.5).int()\n",
        "            tpr.extend(preds.cpu().numpy().flatten().tolist()); ttr.extend(yb.cpu().numpy().flatten().tolist())\n",
        "        tr_loss, tr_acc = np.mean(tloss), np.mean(np.array(tpr)==np.array(ttr))\n",
        "        tr_f1 = f1_score(ttr, tpr)\n",
        "        # valid\n",
        "        model.eval(); vloss=[]; vpr,vtr=[],[]\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                if len(batch)==3:\n",
        "                    xb, mask, yb = [b.to(device) for b in batch]\n",
        "                    logits = model(xb, attention_mask=mask)\n",
        "                else:\n",
        "                    xb, yb = [b.to(device) for b in batch]\n",
        "                    logits = model(xb)\n",
        "                loss=loss_fn(logits, yb); vloss.append(loss.item())\n",
        "                preds=(torch.sigmoid(logits)>0.5).int()\n",
        "                vpr.extend(preds.cpu().numpy().flatten().tolist()); vtr.extend(yb.cpu().numpy().flatten().tolist())\n",
        "        val_loss, val_acc = np.mean(vloss), np.mean(np.array(vpr)==np.array(vtr))\n",
        "        val_f1 = f1_score(vtr, vpr)\n",
        "        print(f\"Epoch {e:02d} | tr_loss {tr_loss:.4f} acc {tr_acc:.4f} f1 {tr_f1:.4f} \"\n",
        "              f\"| val_loss {val_loss:.4f} acc {val_acc:.4f} f1 {val_f1:.4f}\")\n",
        "        sched.step(val_loss)\n",
        "        if val_loss<best: best,wait=val_loss,0\n",
        "        else:\n",
        "            wait+=1\n",
        "            if wait>=patience:\n",
        "                print(f\"Stopping early at epoch {e}\"); break\n",
        "\n",
        "\n",
        "# two pretrained models used\n",
        "if __name__ == \"__main__\":\n",
        "    seq_file = \"expr5Tseq_filtGO_100-1000.lis\"\n",
        "    pos_file = \"GO_3A0055085.annotprot\"\n",
        "    seqs, labs = read(seq_file, pos_file)\n",
        "    (tr_s, tr_l), (va_s, va_l) = generate_train_test(seqs, labs)\n",
        "\n",
        "    batch_size, seq_len = 128, 200\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model_names = [\n",
        "        \"Rostlab/prot_bert\",                       # ProtBERT\n",
        "        \"facebook/esm2_t6_8M_UR50D\",              # ESM-2 small                    # ProtAlbert\n",
        "    ]\n",
        "\n",
        "    for name in model_names:\n",
        "        print(f\"\\n=== Training with {name} ===\")\n",
        "        model = ProteinTransformerClassifier(model_name=name, unfreeze_layers=2)\n",
        "        loader_tr = load_data(batch_size, tr_s, tr_l, seq_len, tokenizer=model.tokenizer)\n",
        "        loader_va = load_data(batch_size, va_s, va_l, seq_len, tokenizer=model.tokenizer)\n",
        "        train_model(model, loader_tr, loader_va, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "IqVxLx3cWyHe",
        "outputId": "2d590f81-07db-46ec-ab37-c3173879ce33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'expr5Tseq_filtGO_100-1000.lis'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a2ab7cd989d8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0mseq_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"expr5Tseq_filtGO_100-1000.lis\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mpos_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"GO_3A0055085.annotprot\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mtr_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mva_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_l\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-a2ab7cd989d8>\u001b[0m in \u001b[0;36mread\u001b[0;34m(seqfile, posfile)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \"\"\"\n\u001b[1;32m     17\u001b[0m     \u001b[0mdatalist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'expr5Tseq_filtGO_100-1000.lis'"
          ]
        }
      ]
    }
  ]
}